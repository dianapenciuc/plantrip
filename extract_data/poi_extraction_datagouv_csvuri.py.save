# Chargement des packages
import requests
import pandas as pd
from io import StringIO
import csv
import numpy as np
from py2neo import Graph, Node, Relationship
from geopy.distance import geodesic
from neo4j import GraphDatabase
import re
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
import time
from geopy.distance import geodesic
from neo4j import GraphDatabase
import matplotlib.pyplot as plt
import seaborn as sns
import geopandas as gpd
from sklearn.cluster import DBSCAN
from scipy.spatial.distance import squareform, pdist
from concurrent.futures import ThreadPoolExecutor, as_completed
from k_means_constrained import KMeansConstrained
from sklearn.preprocessing import MinMaxScaler
import folium
from folium.plugins import MarkerCluster
import matplotlib.colors as mcolors

###########################
# 1/ EXTRACTION DES DONNEES
###########################

# 1/1 Liste URLs par région des CSV de DATATourisme
sigles_regions_urls = [
    ("REU", "Ile de la Réunion", "https://www.data.gouv.fr/fr/datasets/r/2b52bb1f-8676-43f2-b883-f673e7015ed9"),
    ("PDL", "Pays de la Loire", "https://www.data.gouv.fr/fr/datasets/r/56d437a7-eb0c-4c31-9138-539be94bc490"),
    ("PAC", "Provence Alpes Côte d’Azur", "https://www.data.gouv.fr/fr/datasets/r/83a1f131-9e23-4c3b-b1c6-e58f33fe7b80"),
    ("OCC", "Occitanie", "https://www.data.gouv.fr/fr/datasets/r/0c463ef6-c00a-48e2-b50a-d17cfe998b84"),
    ("NOR", "Normandie", "https://www.data.gouv.fr/fr/datasets/r/cb1ebc9c-73fb-43e8-9386-a7b3cf83a642"),
    ("NAQ", "Nouvelle Aquitaine", "https://www.data.gouv.fr/fr/datasets/r/734a4e86-d571-48f6-bdc0-596914066606"),
    ("MYT", "Mayote", "https://www.data.gouv.fr/fr/datasets/r/77114c00-9928-49c1-9a1c-5545c10c7101"),
    ("MTQ", "Martinique", "https://www.data.gouv.fr/fr/datasets/r/4f95a530-d106-4ecd-aa39-1b9d639ee45c"),
    ("IDF", "Ile de France", "https://www.data.gouv.fr/fr/datasets/r/b31a1eca-f2ff-495a-9b67-7c0bc281ea57"),
    ("HDF", "Hauts de France", "https://www.data.gouv.fr/fr/datasets/r/838b6af3-74e5-4d51-873d-d359af3f1855"),
    ("GUF", "Guyane", "https://www.data.gouv.fr/fr/datasets/r/338bb298-cc1f-4bfd-adfd-a3c13fbfa393"),
    ("GLP", "Guadeloupe", "https://www.data.gouv.fr/fr/datasets/r/f73506d6-a336-4743-827b-64a39d891158"),
    ("GDE", "Grand Est", "https://www.data.gouv.fr/fr/datasets/r/59956d74-969b-4c42-8ea4-9348f6a70f7a"),
    ("CVL", "Centre Val de Loire", "https://www.data.gouv.fr/fr/datasets/r/6063e108-f8bd-4541-ba67-a5cadac804fb"),
    ("COR", "Corse", "https://www.data.gouv.fr/fr/datasets/r/2aefffc5-42f5-4e68-ba85-ba19c13fcb4c"),
    ("BRE", "Bretagne", "https://www.data.gouv.fr/fr/datasets/r/ab746af8-d21a-42d1-acae-fdfb2e52ecd5"),
    ("BFC", "Bourgogne Franche Comté", "https://www.data.gouv.fr/fr/datasets/r/d92f0184-e9cb-4bc9-81b0-b43fbcf2a0d2"),
    ("ARA", "Auvergne Rhône Alpes", "https://www.data.gouv.fr/fr/datasets/r/5b3c2cee-44b7-48bd-b4e8-439a03ff6cd2")
]

# 1/2 Ingestion des données de DATAToutisme dans un dataframe

    # 1/2/1 Fonction pour charger les données d'une seule URL

def load_single_url(sigle, region, url, chunk_size=100000):
    data_frames = []
    try:
        response = requests.get(url)
        response.raise_for_status()  # Vérifie si la requête a réussi (statut HTTP 200)
        
        # Utilisation de StringIO pour lire le contenu de la réponse comme un fichier CSV
        csv_data = StringIO(response.text)
        
        # Charger les données par morceaux (chunking)
        chunks = pd.read_csv(csv_data, chunksize=chunk_size, low_memory=False)
        
        for chunk in chunks:
            # Ajouter les colonnes Sigle_Region et Region
            chunk['Sigle_Region'] = sigle
            chunk['Region'] = region
            data_frames.append(chunk)
        
    except requests.exceptions.RequestException as e:
        print(f"Erreur lors du chargement depuis {url}: {str(e)}")
        
    except pd.errors.ParserError as e:
        print(f"Erreur lors de l'analyse du CSV depuis {url}: {str(e)}")
    
    return data_frames

    # 1/2/2 Fonction pour charger les données de toutes les URL en utilisant le multithreading

def load_data_from_urls(sigles_regions_urls, chunk_size=100000, max_workers=10):
    all_data_frames = []
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Soumettre les tâches au thread pool
        futures = [executor.submit(load_single_url, sigle, region, url, chunk_size) for sigle, region, url in sigles_regions_urls]
        
        # Attendre que toutes les tâches soient terminées
        for future in as_completed(futures):
            all_data_frames.extend(future.result())
    
    # Concaténer tous les DataFrames en un seul
    concatenated_df = pd.concat(all_data_frames, join='inner', ignore_index=True)
    
    return concatenated_df

concatenated_df = load_data_from_urls(sigles_regions_urls)


###############################
# 2/ TRANSFORMATION DES DONNEES
###############################


# 2/1 Séparation de la variable Code_postal_et_commune en deux champs 'code postal' et 'commune'

concatenated_df[['code_postal', 'commune']] = concatenated_df['Code_postal_et_commune'].str.split('#', expand=True)


# 2/2 Identification de la véritable variable 'catégorie'


      # 2/2/1 Sélection des variables utiles pour la suite de l'analyse  

concatenated_df_cleaned_util=concatenated_df[['URI_ID_du_POI','Nom_du_POI', 'Description', 'Categories_de_POI','Latitude','Longitude', 'Adresse_postale', 'code_postal', 'commune', 'Region', 'Sigle_Region', 'Date_de_mise_a_jour', 'Contacts_du_POI']]

      # 2/2/2 liste des éléments rédondants et non significatives à supprimer

to_remove = ["https://www.datatourisme.fr/ontology/core#","http://schema.org/","http://purl.org/ontology/olo/core#"]

      # 2/2/3 Fonction pour nettoyer les caratères spéciaux

def clean_string(input_string):
    for item in to_remove:
        input_string = input_string.replace(item, "")
    input_string = input_string.replace("||", "|").strip("|")
    return input_string

      # 2/2/4 Appliquer la fonction de nettoyage à la colonne 'Categorie_POi'

concatenated_df_cleaned_util["Categories_de_POI_new"] = concatenated_df_cleaned_util['Categories_de_POI'].apply(clean_string)

      # 2/2/5 Liste des catégories voulues

target_categories = ['PlaceOfInterest', 'Product', '%Event%', '%Tour%']

      # 2/2/6 Fonction pour assigner les catégories voulues

def get_categorie_ok(categories, target_categories):
    for category in target_categories:
        regex_category = category.replace('%', '.*')
        if any(re.search(regex_category, cat) for cat in categories):
            return category
    return categories[0] if categories else None

      # 2/2/7 Appliquer la fonction aux données

concatenated_df_cleaned_util['Category_List'] = concatenated_df_cleaned_util['Categories_de_POI_new'].str.split('|')
concatenated_df_cleaned_util['Catégorie_OK'] = concatenated_df_cleaned_util['Category_List'].apply(lambda x: get_categorie_ok(x, target_categories))

      # 2/2/8 Dictionnaire de mapping entre les catégories voulues et les labels de celles-ci

mapping = {'PlaceOfInterest': 'Lieu', '%Event%': 'Evènement et manifestation', '%Tour%': 'Itinéraire touristique', 'Product': 'Produit'}

      # 2/2/9 Remplacement des catégories voulues par leur label

concatenated_df_cleaned_util['Catégorie_OK'] = concatenated_df_cleaned_util['Catégorie_OK'].replace(mapping)
concatenated_df_cleaned_util["Catégorie_OK"].value_counts()


# 2/3 Identification du thème


      # 2/3/1 Suppression des espaces inutiles

concatenated_df_cleaned_util['Categories_de_POI_temp'] = concatenated_df_cleaned_util['Categories_de_POI_new'].str.strip()

      # 2/3/2 Remplacement des valeurs manquantes par une chaîne vide

concatenated_df_cleaned_util['Categories_de_POI_temp'] = concatenated_df_cleaned_util['Categories_de_POI_temp'].fillna('')

      # 2/3/3 Liste des catégories à supprimer

categories_to_remove = ['PlaceOfInterest','PointOfInterest', 'Product', 'Event', 'Tour']

      # 2/3/4 Fonction pour supprimer les occurrences spécifiées

def remove_categories_and_deduplicate(categories, to_remove):
    categories = categories.split('|')
    filtered_categories = [category for category in categories if not any(rem in category for rem in to_remove)]
    unique_categories = list(dict.fromkeys(filtered_categories))  # Éliminer les doublons en conservant l'ordre
    return '|'.join(unique_categories)

      # 2/3/5 Appliquer la fonction à la colonne 'Categories' pour créer 'Categorie_temp'

concatenated_df_cleaned_util['Categorie_temp'] = concatenated_df_cleaned_util['Categories_de_POI_temp'].apply(lambda x: remove_categories_and_deduplicate(x, categories_to_remove))

      # 2/3/6 Sélectionner la dernière valeur de chaque chaîne dans Categorie_temp

concatenated_df_cleaned_util['Thème_OK'] = concatenated_df_cleaned_util['Categorie_temp'].apply(lambda x: x.split('|')[-1])
concatenated_df_cleaned_util['Thème_OK'].value_counts()


############################################################
# 3/ CHARGEMENT DES DONNEES DE LA REGION DE CORSE DANS NEO4J
############################################################


# 3/1 Les noeuds du graphe et leur attributs

      # 3/1/1 Sélection des variables qui serviront d'attributs pour les noeuds du graphe

concatenated_df_cleaned_util_pour_graph=concatenated_df_cleaned_util[['Nom_du_POI', 'Description', 'Catégorie_OK', 'Adresse_postale', 'code_postal', 'commune', 'Region', 'Sigle_Region', 'Thème_OK', 'Latitude','Longitude']]

      # 3/1/2 Sélection des données de la Corse (COR) qui représenteront les noeuds du graphe

filtered_df_Lieu_Corse = concatenated_df_cleaned_util_pour_graph[(concatenated_df_cleaned_util_pour_graph['Region'] == 'Corse') & (concatenated_df_cleaned_util_pour_graph['Catégorie_OK'] == 'Lieu')]
filtered_df_Lieu_Corse.head()

# 3/2 Les relations du graphe et leurs attributs

      # 3/2/1 Fonction pour calculer la distance entre deux POIs (en kilomètres)

def calculate_distance(poi1, poi2):
    lat1, lon1 = poi1['Latitude'], poi1['Longitude']
    lat2, lon2 = poi2['Latitude'], poi2['Longitude']
    return poi1['Nom_du_POI'], poi2['Nom_du_POI'], geodesic((lat1, lon1), (lat2, lon2)).kilometers

      # 3/2/2 Liste pour stocker les paires de POI et les distances qui les séparent

poi_pairs = []
distances = []

      # 3/2/3 Fonction pour traiter un chunk de données

def process_chunk(start_idx, end_idx):
    local_pairs = []
    local_distances = []
    for i in range(start_idx, end_idx):
        for j in range(i + 1, len(filtered_df_Lieu_Corse)):
            poi1 = filtered_df_Lieu_Corse.iloc[i]
            poi2 = filtered_df_Lieu_Corse.iloc[j]
            poi1_name, poi2_name, distance = calculate_distance(poi1, poi2)
            local_pairs.append((poi1_name, poi2_name))
            local_distances.append(distance)
    return local_pairs, local_distances

      # 3/2/4 Mesure du temps de début global

total_start_time = time.time()

      # 3/2/5 Mesure du temps de début du calcul des distances
start_time_distances = time.time()

      # 3/2/6 Utilisation de ThreadPoolExecutor pour paralléliser les calculs

num_threads = 4  
chunk_size = len(filtered_df_Lieu_Corse) // num_threads

with ThreadPoolExecutor(max_workers=num_threads) as executor:
    futures = [executor.submit(process_chunk, i, i + chunk_size) for i in range(0, len(filtered_df_Lieu_Corse), chunk_size)]
    for future in futures:
        local_pairs, local_distances = future.result()
        poi_pairs.extend(local_pairs)
        distances.extend(local_distances)

      # 3/2/7 Mesure du temps de fin du calcul des distances

end_time_distances = time.time()

      # 3/2/8 Création du DataFrame des relations

distance_df = pd.DataFrame(poi_pairs, columns=['POI1', 'POI2'])
distance_df['distance_POI1_POI2'] = distances

      # 3/2/9 Mesure du temps de début de la normalisation de la distance entre POI

start_time_normalization = time.time()

      # 3/2/10 Normalisation de la distance entre POI avec Min-Max Scaling

scaler = MinMaxScaler()
distance_df['normalized_distance'] = scaler.fit_transform(distance_df[['distance_POI1_POI2']])

      # 3/2/11 Mesure du temps de fin de la normalisation

end_time_normalization = time.time()

      # 3/2/12 Mesure du temps de fin global

total_end_time = time.time()

      # 3/2/13 Affichage du DataFrame et des temps pris

print(f"Temps pris pour calculer les distances: {end_time_distances - start_time_distances} secondes")
print(f"Temps pris pour la normalisation des distances: {end_time_normalization - start_time_normalization} secondes")
print(f"Temps total pris pour le traitement: {total_end_time - total_start_time} secondes")
distance_df.head()

# 3/3 Analyse de la distance entre les POIs


      # 3/3/1 Définir le style de seaborn pour les graphiques

sns.set(style="whitegrid")

      # 3/3/2 Créer l'histogramme des scores

plt.figure(figsize=(10, 6))
sns.histplot(distance_df['distance_POI1_POI2'], bins=20, kde=True, color='skyblue')
plt.title('Distribution des distances entre POIs en Corse')
plt.xlabel('Distance')
plt.ylabel('Nombre de couples de POIs')
plt.tight_layout()

      # 3/3/3 Afficher le graphique
plt.show()

# 3/4 Chargement des noeuds et des relations dans Neo4j

      # 3/4/1 Connexion à Neo4j

graph = Graph("bolt://34.241.34.249:7687", auth=("neo4j", "neo4j")) # l'adresse IP du server Neo4j doit être adaptée

      # 3/4/2 Fonction pour créer un nœud POI

def create_node(row):
    query = (
        f'MERGE (poi:POI {{name: "{row["Nom_du_POI"]}", description: "{row["Description"]}", '
        f'category: "{row["Catégorie_OK"]}", address: "{row["Adresse_postale"]}", '
        f'postal_code: "{row["code_postal"]}", commune: "{row["commune"]}", '
        f'region: "{row["Region"]}", region_sigle: "{row["Sigle_Region"]}", '
        f'theme: "{row["Thème_OK"]}", latitude: {row["Latitude"]}, longitude: {row["Longitude"]}}})'
    )
    graph.run(query)

      # 3/4/3 Fonction pour créer une relation entre les nœuds POI

def create_relationship(row):
    query = (
        f'MATCH (poi1:POI {{name: "{row["POI1"]}"}}), (poi2:POI {{name: "{row["POI2"]}"}}) '
        f'MERGE (poi1)-[:EST_DISTANT_DE {{distance: {row["distance_POI1_POI2"]}, distance_normalized: {row["normalized_distance"]}}}]->(poi2)'
    )
    graph.run(query)

      # 3/4/4 Fonction pour traiter les nœuds en parallèle

def process_nodes_parallel():
    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(create_node, [row for _, row in filtered_df_Lieu_Corse.iterrows()])

      # 3/4/5 Fonction pour traiter les relations en parallèle

def process_relationships_parallel():
    with ThreadPoolExecutor(max_workers=10) as executor:
        executor.map(create_relationship, [row for _, row in distance_df.iterrows()])

      # 3/4/6 Mesurer le temps de création des nœuds

start_time_nodes = time.time()
process_nodes_parallel()
end_time_nodes = time.time()
print(f"Temps pris pour charger les nœuds: {end_time_nodes - start_time_nodes} secondes")

      # 3/4/7 Mesurer le temps de création des relations

start_time_relations = time.time()
process_relationships_parallel()
end_time_relations = time.time()
print(f"Temps pris pour charger les relations: {end_time_relations - start_time_relations} secondes")

print("Chargement des nœuds et des relations entre POI dans Neo4j terminé avec succès.")










